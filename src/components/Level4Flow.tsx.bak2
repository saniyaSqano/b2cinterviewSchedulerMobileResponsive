import React, { useState, useRef, useEffect, useCallback } from 'react';
import { ArrowLeft, Video, VideoOff, Mic, MicOff, Download, Square, Circle, Upload, Check, User, Mail, Phone, Award, Star, TrendingUp, Clock, AlertTriangle } from 'lucide-react';
import Level4CongratulationsScreen from './Level4CongratulationsScreen';
import VideoFeed from './VideoFeed';
import SelfPracticeReport from './SelfPracticeReport';
import { uploadMockVideo } from '../utils/s3Service';
import jsPDF from 'jspdf';
import * as faceDetection from '../utils/mediapipeFaceDetection';
import { setupProctorEventListeners, ProctorEventType } from '../utils/proctorUtils';
import { startFaceDetectionMonitoring, FaceDetectionEventType } from '../utils/faceDetectionUtils';

interface Level4FlowProps {
  onBack: () => void;
  userName: string;
}

interface ViolationLog {
  id: string;
  type: 'warning' | 'info' | 'error';
  message: string;
  timestamp: string;
  duration?: string; // Duration formatted as string (e.g., "3s")
}

const Level4Flow = ({ onBack, userName }: Level4FlowProps): React.ReactElement => {
  const [showCongratulations, setShowCongratulations] = useState(true);
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
  const [isComplete, setIsComplete] = useState(false);
  const [isVideoOn, setIsVideoOn] = useState(false);
  const [isMicOn, setIsMicOn] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [recordedChunks, setRecordedChunks] = useState<Blob[]>([]);
  const [violationLogs, setViolationLogs] = useState<ViolationLog[]>([]);
  const [hasRecording, setHasRecording] = useState(false);
  const [isUploading, setIsUploading] = useState(false);
  const [uploadSuccess, setUploadSuccess] = useState(false);
  const [uploadUrl, setUploadUrl] = useState<string>('');
  const [showReport, setShowReport] = useState(false);
  const [showCustomReport, setShowCustomReport] = useState(false);
  
  // Face detection state
  const [isFaceDetectionInitialized, setIsFaceDetectionInitialized] = useState(false);
  const [facePresent, setFacePresent] = useState(true);
  const [faceDetectionStatus, setFaceDetectionStatus] = useState<'initializing' | 'active' | 'error'>('initializing');
  
  // Store user details from localStorage
  const [userDetails, setUserDetails] = useState({
    fullName: userName,
    email: "",
    phoneNumber: "",
    skills: "",
    experience: ""
  });
  
  // Load user data from localStorage when component mounts
  useEffect(() => {
    const storedUserData = localStorage.getItem('currentUserData');
    if (storedUserData) {
      try {
        const userData = JSON.parse(storedUserData);
        setUserDetails({
          fullName: userData.fullName || userName,
          email: userData.email || "",
          phoneNumber: userData.phoneNumber || "",
          skills: userData.skills || "",
          experience: userData.experience || ""
        });
      } catch (error) {
        console.error('Error parsing user data from localStorage:', error);
      }
    }
  }, [userName]);
  
  // These refs are already declared elsewhere in the component
  
  // Track when the stream becomes available
  const [isStreamAvailable, setIsStreamAvailable] = useState(false);

  // This function will be called by VideoFeed when the stream is ready
  const handleStreamReady = useCallback((stream: MediaStream) => {
    console.log('Stream is now available for face detection');
    streamRef.current = stream;
    setIsStreamAvailable(true);
  }, []);
  
  // Enhanced video status change handler
  const handleVideoStatusChange = useCallback((isOn: boolean) => {
    console.log(`Video status changed: ${isOn ? 'ON' : 'OFF'}`);
    setIsVideoOn(isOn);
    
    // If video is turned off, reset face detection
    if (!isOn) {
      setIsFaceDetectionInitialized(false);
      setFaceDetectionStatus('error'); // Use 'error' instead of 'inactive'
      setIsStreamAvailable(false);
      
      // Clear any face detection intervals
      if (faceDetectionIntervalRef.current) {
        clearInterval(faceDetectionIntervalRef.current);
        faceDetectionIntervalRef.current = null;
      }
    }
  }, []);
  
  // Initialize face detection when video is turned on AND stream is available
  useEffect(() => {
    if (isVideoOn && isStreamAvailable && faceDetectionVideoRef.current && streamRef.current) {
      console.log('Initializing MediaPipe face detection...');
      setFaceDetectionStatus('initializing');
      
      // Connect the stream to the face detection video element
      if (streamRef.current && faceDetectionVideoRef.current) {
        console.log('Connecting stream to face detection video element in useEffect');
        
        // Only set srcObject if it's not already set
        if (faceDetectionVideoRef.current.srcObject !== streamRef.current) {
          faceDetectionVideoRef.current.srcObject = streamRef.current;
          
          // Make sure the video is playing
          faceDetectionVideoRef.current.play().catch(err => {
            console.error('Error playing face detection video:', err);
          });
        }
      } else {
        console.error('No stream available for face detection');
        setFaceDetectionStatus('error');
        return;
      }
      
      const initDetection = async () => {
        try {
          // Initialize MediaPipe face detection
          const success = await faceDetection.initFaceDetection();
          
          if (success) {
            setIsFaceDetectionInitialized(true);
            setFaceDetectionStatus('active');
            console.log('MediaPipe face detection initialized successfully');
            
            // Set up callbacks for face detection events
            faceDetection.setFaceDetectionCallbacks({
              onFaceLost: (duration) => {
                console.log(`Face lost for ${duration}ms`);
                setFacePresent(false);
                setFaceDetectionStatus('error');
                addViolationLog(
                  'error',
                  'Face not detected in camera view',
                  duration > 1000 ? `${Math.round(duration / 1000)}s` : undefined
                );
              },
              onFaceReturned: () => {
                console.log('Face returned to camera view');
                setFacePresent(true);
                setFaceDetectionStatus('active');
                addViolationLog(
                  'info',
                  'Face returned to camera view',
                  undefined
                );
              }
            });
            
            // Wait for the video to be ready before initial detection
            if (faceDetectionVideoRef.current.readyState >= 2) {
              try {
                const initialDetection = await faceDetection.detectFaces(faceDetectionVideoRef.current);
                console.log('Initial face detection result:', initialDetection);
              } catch (detectionError) {
                console.warn('Initial detection test failed, but continuing:', detectionError);
              }
            } else {
              console.log('Video not ready yet, waiting for loadeddata event');
              // Wait for video to be loaded before initial detection
              faceDetectionVideoRef.current.addEventListener('loadeddata', async () => {
                try {
                  const initialDetection = await faceDetection.detectFaces(faceDetectionVideoRef.current);
                  console.log('Initial face detection result (after video loaded):', initialDetection);
                } catch (detectionError) {
                  console.warn('Initial detection test failed after video loaded:', detectionError);
                }
              }, { once: true });
            }
          } else {
            console.error('Failed to initialize MediaPipe face detection');
            setFaceDetectionStatus('error');
            setIsFaceDetectionInitialized(false);
            addViolationLog(
              'error',
              'Face detection failed to initialize',
              undefined
            );
          }
        } catch (error) {
          console.error('Error during MediaPipe face detection initialization:', error);
          setFaceDetectionStatus('error');
          setIsFaceDetectionInitialized(false);
          addViolationLog(
            'error',
            'Face detection initialization failed',
            undefined
          );
        }
      };
      
      // Give video element time to initialize before starting face detection
      setTimeout(() => {
        initDetection();
      }, 1000);
    } else {
      setIsFaceDetectionInitialized(false);
    }
    
    // Cleanup function
    return () => {
      if (faceDetectionIntervalRef.current) {
        clearInterval(faceDetectionIntervalRef.current);
        faceDetectionIntervalRef.current = null;
      }
    };
  }, [isVideoOn]);

  
  // Set up face detection monitoring when face detection is initialized and stream is available
  useEffect(() => {
    if (isVideoOn && isFaceDetectionInitialized && isStreamAvailable && faceDetectionVideoRef.current && streamRef.current) {
      console.log('Setting up face detection monitoring interval');
      
      // Double check that video element is properly set up with the stream
      if (faceDetectionVideoRef.current.srcObject !== streamRef.current) {
        console.log('Reconnecting stream to face detection video element before monitoring');
        faceDetectionVideoRef.current.srcObject = streamRef.current;
        
        // Make sure the video is playing
        faceDetectionVideoRef.current.play().catch(err => {
          console.error('Error playing face detection video:', err);
        });
      }
      
  const videoRef = useRef<HTMLVideoElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const faceDetectionVideoRef = useRef<HTMLVideoElement>(null);
  const faceDetectionIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  
  // Set up face detection monitoring when face detection is initialized and stream is available
  useEffect(() => {
    if (isVideoOn && isFaceDetectionInitialized && isStreamAvailable && faceDetectionVideoRef.current && streamRef.current) {
      console.log('Starting face detection monitoring with improved utility');
      setFaceDetectionStatus('initializing');
      
      // Clear any existing interval
      if (faceDetectionIntervalRef.current) {
        clearInterval(faceDetectionIntervalRef.current);
        faceDetectionIntervalRef.current = null;
      }
      
      // Start face detection monitoring with the new utility
      const cleanupFaceDetection = startFaceDetectionMonitoring(
        faceDetectionVideoRef.current,
        (eventType: FaceDetectionEventType, details: string) => {
          console.log(`Face detection event: ${eventType} - ${details}`);
          
          // Handle different face detection events
          switch (eventType) {
            case 'no_face':
              // Update UI state for no face
              setFacePresent(false);
              setFaceDetectionStatus('error');
              
              // Log violation
              addViolationLog(
                'error',
                'Face not detected in camera view',
                undefined
              );
              break;
              
            case 'multiple_faces':
              // Update UI state for multiple faces
              setFacePresent(false);
              setFaceDetectionStatus('error');
              
              // Log violation
              addViolationLog(
                'warning',
                details,
                undefined
              );
              break;
              
            case 'face_returned':
              // Update UI state for face returned
              setFacePresent(true);
              setFaceDetectionStatus('active');
              
              // Log info
              addViolationLog(
                'info',
                'Face detected in camera view',
                undefined
              );
              break;
          }
        }
      );
      
      console.log('Face detection monitoring started with improved utility');
      
      // Return cleanup function
      return () => {
        cleanupFaceDetection();
        if (faceDetectionIntervalRef.current) {
          clearInterval(faceDetectionIntervalRef.current);
          faceDetectionIntervalRef.current = null;
        }
      };
    } else {
      // Clear interval when video is off or face detection is not initialized
      if (faceDetectionIntervalRef.current) {
        clearInterval(faceDetectionIntervalRef.current);
        faceDetectionIntervalRef.current = null;
      }
    }
  }, [isFaceDetectionInitialized, isVideoOn, isStreamAvailable, addViolationLog]);

  const questions = [
    "Welcome to your self-practice session! Let's start with a warm-up. Tell me about yourself and what brings you here today.",
    "Describe a challenging situation you faced in your previous role and how you handled it.",
    "What are your greatest strengths and how do they apply to this position?",
    "Where do you see yourself in 5 years and how does this role fit into your career goals?",
    "Do you have any questions about the role or our company?"
  ];

  // Function to add a violation log
  const addViolationLog = (
    type: 'error' | 'warning' | 'info',
    message: string,
    duration?: number | string,
  ) => {
    const newLog: ViolationLog = {
      id: `violation-${Date.now()}`,
      timestamp: new Date().toLocaleTimeString(),
      type,
      message,
      duration: typeof duration === 'number' ? `${Math.round(duration / 1000)}s` : duration as string | undefined,
    };
    
    console.log('Adding violation log:', newLog);
    
    // Force immediate update of violation logs
    setViolationLogs((prev) => {
      const updatedLogs = [newLog, ...prev].slice(0, 20); // Limit to 20 logs to prevent memory issues
      console.log('Updated violation logs:', updatedLogs);
      return updatedLogs;
    });
    
    // Force a re-render to update the UI
    setFaceDetectionStatus(prev => {
      console.log('Forcing UI update after violation log');
      return prev;
    });
  };

  useEffect(() => {
    console.log('Video status changed:', isVideoOn ? 'ON' : 'OFF');
  }, [isVideoOn]);
  
  // Set up proctor event listeners for tab changes and keyboard shortcuts
  useEffect(() => {
    if (isVideoOn) {
      console.log('Setting up proctor event listeners for tab changes and keyboard shortcuts');
      
      // Set up event listeners and get cleanup function
      const cleanupListeners = setupProctorEventListeners((eventType: ProctorEventType, details: string) => {
        console.log(`Proctor event detected: ${eventType} - ${details}`);
        
        // Log different types of violations based on the event type
        switch (eventType) {
          case 'tab_change':
            addViolationLog(
              'error',
              'Tab change detected - User switched away from the interview',
              undefined
            );
            break;
          
          case 'keyboard_shortcut':
            addViolationLog(
              'warning',
              `Prohibited keyboard shortcut used: ${details}`,
              undefined
            );
            break;
            
          case 'window_blur':
            addViolationLog(
              'warning',
              'User clicked outside the interview window',
              undefined
            );
            break;
        }
      });
      
      // Clean up event listeners when component unmounts or interview ends
      return cleanupListeners;
    }
  }, [isVideoOn, addViolationLog]);

  const handleProceedToInterview = () => {
    setShowCongratulations(false);
  };
  
  // The enhanced handleVideoStatusChange is now defined earlier in the component

  const startRecording = async () => {
    if (!isVideoOn) {
      console.error('Video must be enabled to start recording');
      alert('Please enable your camera before recording');
      return;
    }

    try {
      // Get both video and audio streams
      let combinedStream;
      
      if (streamRef.current) {
        // If we already have the video stream from VideoFeed
        const videoTracks = streamRef.current.getVideoTracks();
        
        if (videoTracks.length === 0) {
          console.error('No video tracks available in the current stream');
          alert('Camera not properly initialized. Please refresh and try again.');
          return;
        }
        
        // Get audio stream separately if needed
        let audioStream;
        try {
          audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          setIsMicOn(true);
        } catch (audioError) {
          console.warn('Could not get audio stream:', audioError);
          alert('Could not access microphone. Recording will proceed without audio.');
        }
        
        // Combine video and audio tracks
        const tracks = [...videoTracks];
        if (audioStream) {
          tracks.push(...audioStream.getAudioTracks());
        }
        
        combinedStream = new MediaStream(tracks);
      } else {
        // If we don't have a stream yet, get both video and audio
        combinedStream = await navigator.mediaDevices.getUserMedia({
          video: true,
          audio: true
        });
        streamRef.current = combinedStream;
        setIsVideoOn(true);
        setIsMicOn(true);
      }

      console.log('Combined stream created with tracks:', combinedStream.getTracks().map(t => t.kind));
      
      // Create media recorder with the combined stream
      const mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType: 'video/webm;codecs=vp8,opus'
      });

      setRecordedChunks([]);

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          setRecordedChunks(prev => [...prev, event.data]);
          console.log('Recorded chunk size:', event.data.size);
        }
      };

      mediaRecorder.onstop = () => {
        setHasRecording(true);
        console.log('Recording stopped, total chunks:', recordedChunks.length);
        
        // Wait a short time to ensure all chunks are collected
        setTimeout(async () => {
          console.log('Preparing to upload recording, chunks:', recordedChunks.length);
          
          if (recordedChunks.length > 0) {
            try {
              setIsUploading(true);
              setUploadSuccess(false);
              
              // Create blob from recorded chunks
              const blob = new Blob(recordedChunks, { type: 'video/webm' });
              console.log('Blob size for upload:', blob.size, 'bytes');
              
              if (blob.size === 0) {
                console.error('Recording is empty');
                setIsUploading(false);
                return;
              }
              
              // Generate filename with user name and timestamp
              const fileName = `self-practice-interview-${userName.replace(/\s+/g, '-')}-${new Date().toISOString().split('T')[0]}.webm`;
              
              // Upload to S3 using the uploadMockVideo function
              console.log('Automatically uploading recording to S3:', fileName);
              const downloadUrl = await uploadMockVideo(blob, fileName);
              
              setUploadUrl(downloadUrl);
              setUploadSuccess(true);
              setIsUploading(false);
              
              console.log('Automatic upload successful, download URL:', downloadUrl);
              alert('Your recording has been successfully uploaded to S3 and is available for viewing.');
            } catch (error) {
              console.error('Error during automatic upload to S3:', error);
              alert('There was an error uploading your recording to S3: ' + error.message);
              setIsUploading(false);
            }
          }
        }, 1500);
      };

      mediaRecorder.start(1000); // Collect data every second
      setIsRecording(true);
      console.log('Recording started successfully');
      
      // Reset to first question when recording starts
      setCurrentQuestionIndex(0);
      
    } catch (error) {
      console.error('Failed to start recording:', error);
      alert('Failed to start recording: ' + error.message);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      try {
        mediaRecorderRef.current.stop();
        setIsRecording(false);
        console.log('Stopping recording');
        
        // Release tracks to avoid memory leaks
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => {
            // Only stop audio tracks, keep video running for the UI
            if (track.kind === 'audio') {
              track.stop();
            }
          });
        }
        
        // Show a notification that recording has stopped
        alert('Recording has been stopped. It will be automatically uploaded to S3.');
      } catch (error) {
        console.error('Error stopping recording:', error);
      }
    }
  };

  const downloadRecording = () => {
    if (recordedChunks.length === 0) {
      console.error('No recording available');
      alert('No recording available to download');
      return;
    }

    try {
      console.log('Creating blob from', recordedChunks.length, 'chunks');
      const blob = new Blob(recordedChunks, { type: 'video/webm' });
      console.log('Blob size:', blob.size, 'bytes');
      
      if (blob.size === 0) {
        alert('Recording is empty. Please try recording again.');
        return;
      }
      
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `self-practice-interview-${userName.replace(/\s+/g, '-')}-${new Date().toISOString().split('T')[0]}.webm`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
      
      console.log('Download initiated');
    } catch (error) {
      console.error('Error downloading recording:', error);
      alert('Error downloading recording: ' + error.message);
    }
  };

  const uploadRecordingToS3 = async () => {
    if (recordedChunks.length === 0) {
      console.error('No recording available');
      alert('No recording available to upload');
      return;
    }

    try {
      setIsUploading(true);
      setUploadSuccess(false);
      
      console.log('Creating blob for S3 upload from', recordedChunks.length, 'chunks');
      const blob = new Blob(recordedChunks, { type: 'video/webm' });
      console.log('Blob size for upload:', blob.size, 'bytes');
      
      if (blob.size === 0) {
        alert('Recording is empty. Please try recording again.');
        setIsUploading(false);
        return;
      }
      
      // Generate filename with user name and timestamp
      const fileName = `self-practice-interview-${userName.replace(/\s+/g, '-')}-${new Date().toISOString().split('T')[0]}.webm`;
      
      // Upload to S3 using the uploadMockVideo function
      console.log('Uploading recording to S3:', fileName);
      const downloadUrl = await uploadMockVideo(blob, fileName);
      
      setUploadUrl(downloadUrl);
      setUploadSuccess(true);
      setIsUploading(false);
      
      console.log('Upload successful, download URL:', downloadUrl);
      alert('Recording has been successfully uploaded to S3. It can be accessed using the provided link.');
    } catch (error) {
      console.error('Error uploading recording to S3:', error);
      alert('Error uploading recording: ' + error.message);
      setIsUploading(false);
    }
  };

  if (showCongratulations) {
    return (
      <Level4CongratulationsScreen
        onBack={onBack}
        onProceed={handleProceedToInterview}
        userName={userName}
      />
    );
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 via-indigo-50 to-purple-50">
      {/* Header */}
      <div className="bg-white/95 backdrop-blur-sm border-b border-gray-200 p-4 shadow-sm">
        <div className="flex items-center justify-between max-w-7xl mx-auto">
          <div className="flex items-center space-x-4">
            <button
              onClick={onBack}
              className="p-2 rounded-full bg-gray-100 hover:bg-gray-200 transition-colors"
            >
              <ArrowLeft className="w-5 h-5 text-gray-700" />
            </button>
            <div>
              <h1 className="text-2xl font-bold text-gray-800">Self-Practice Interview</h1>
              <p className="text-sm text-gray-600">Level 4 - Practice Session</p>
            </div>
          </div>
          
          {/* Recording Controls */}
          <div className="flex items-center space-x-3">
            {!isRecording ? (
              <button
                onClick={startRecording}
                disabled={!isVideoOn}
                className="flex items-center space-x-2 px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 disabled:opacity-50 disabled:cursor-not-allowed transition-colors font-medium shadow-md"
              >
                <Circle className="w-4 h-4 animate-pulse" />
                <span>Start Recording</span>
              </button>
            ) : (
              <button
                onClick={stopRecording}
                className="flex items-center space-x-2 px-4 py-2 bg-red-600 text-white rounded-lg hover:bg-red-700 transition-colors animate-pulse shadow-md"
              >
                <Square className="w-4 h-4" />
                <span>Stop Recording</span>
              </button>
            )}
            
            {hasRecording && (
              <div className="flex space-x-2">
                <button
                  onClick={downloadRecording}
                  className="flex items-center space-x-2 px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition-colors font-medium shadow-md"
                >
                  <Download className="w-4 h-4" />
                  <span>Download Recording</span>
                </button>
                
                {isUploading ? (
                  <div className="flex items-center space-x-2 px-4 py-2 bg-green-500 text-white rounded-lg font-medium shadow-md">
                    <div className="animate-spin w-4 h-4 border-2 border-white border-t-transparent rounded-full" />
                    <span>Uploading to S3...</span>
                  </div>
                ) : uploadSuccess ? (
                  <button
                    onClick={() => window.open(uploadUrl, '_blank')}
                    className="flex items-center space-x-2 px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition-colors font-medium shadow-md"
                  >
                    <Check className="w-4 h-4" />
                    <span>View on S3</span>
                  </button>
                ) : null}
              </div>
            )}
            
            <button
              onClick={() => setIsComplete(true)}
              className="px-4 py-2 bg-gray-500 text-white rounded-lg hover:bg-gray-600 transition-colors shadow-md"
            >
              End Session
            </button>
          </div>
        </div>
      </div>

      <div className="flex h-screen pt-20">
        {/* Left Side - Video Feed */}
        <div className="w-1/2 p-6">
          <div className="bg-white rounded-2xl shadow-lg p-6 h-full">
            <div className="flex items-center justify-between mb-4">
              <h3 className="text-lg font-semibold text-gray-800">Your Video</h3>
              <div className="flex items-center space-x-2">
                {isRecording && (
                  <div className="flex items-center space-x-2 text-red-500">
                    <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
                    <span className="text-sm font-medium">Recording</span>
                  </div>
                )}
              </div>
            </div>
            
            <div className="relative h-[calc(100%-4rem)] bg-gray-900 rounded-xl overflow-hidden">
              <VideoFeed
                onStatusChange={handleVideoStatusChange}
                videoRef={videoRef}
                faceDetectionVideoRef={faceDetectionVideoRef}
                onStreamReady={handleStreamReady}
              />
              {/* Hidden video element for face detection */}
              <video 
                ref={faceDetectionVideoRef} 
                className="hidden" 
                playsInline 
                muted 
                autoPlay
                width="640"
                height="480"
                onLoadedData={() => console.log('Face detection video loaded')}
                onError={(e) => console.error('Face detection video error:', e)}
              />
              
              <div className="absolute bottom-4 right-4 flex items-center space-x-2 bg-black bg-opacity-50 text-white text-xs px-2 py-1 rounded">
                <div className={`w-3 h-3 rounded-full ${isVideoOn ? 'bg-green-500' : 'bg-red-500'}`}></div>
                <span>Camera {isVideoOn ? 'Active' : 'Inactive'}</span>
              </div>
              
              {/* Face detection status indicator */}
              {isVideoOn && (
                <div className={`absolute bottom-4 left-4 flex items-center space-x-2 ${!facePresent ? 'bg-red-500' : 'bg-black bg-opacity-50'} text-white text-xs px-2 py-1 rounded ${!facePresent ? 'animate-pulse' : ''}`}>
                  <div className={`w-3 h-3 rounded-full ${facePresent ? 'bg-green-500' : 'bg-white'}`}></div>
                  <span>Face {facePresent ? 'Detected' : 'NOT DETECTED'}</span>
                </div>
              )}
              
              {/* Large warning when face is not detected */}
              {isVideoOn && !facePresent && (
                <div className="absolute inset-0 flex items-center justify-center bg-black bg-opacity-50">
                  <div className="bg-red-600 text-white p-4 rounded-lg shadow-lg animate-pulse text-center max-w-md">
                    <AlertTriangle className="w-12 h-12 mx-auto mb-2" />
                    <h3 className="text-xl font-bold mb-2">Face Not Detected</h3>
                    <p>Please position your face clearly in the camera view.</p>
                    <p className="text-sm mt-2">This violation is being logged.</p>
                  </div>
                </div>
              )}
            </div>
          </div>
        </div>

        {/* Right Side - AI Interview */}
        <div className="w-1/2 p-6 flex flex-col">
          {/* Current Question Display */}
          <div className="bg-indigo-50 p-4 rounded-lg mb-4">
            <h3 className="text-sm font-medium text-indigo-800 mb-1">Current Question ({currentQuestionIndex + 1}/{questions.length})</h3>
            <p className="text-gray-800">{questions[currentQuestionIndex]}</p>
          </div>

          {/* AI Practice Coach */}
          <div className="bg-white rounded-2xl shadow-lg p-6 mb-6">
            <div className="flex items-center space-x-3 mb-4">
              <div className="w-12 h-12 bg-gradient-to-br from-blue-500 to-purple-500 rounded-full flex items-center justify-center">
                <span className="text-white font-bold text-lg">AI</span>
              </div>
              <div>
                <h3 className="text-lg font-semibold text-gray-800">AI Practice Coach</h3>
                <p className="text-sm text-gray-600">Your interview companion</p>
                <div className="flex items-center space-x-1 mt-1">
                  <div className="w-2 h-2 bg-green-500 rounded-full"></div>
                  <span className="text-xs text-green-600">Online</span>
                </div>
              </div>
            </div>

            {/* Current Question */}
            <div className="bg-gray-50 rounded-xl p-4">
              <div className="flex items-center justify-between mb-2">
                <span className="text-sm font-medium text-blue-600">Question {currentQuestionIndex + 1} of {questions.length}</span>
                <span className="text-sm text-gray-500">11:40</span>
              </div>
              <p className="text-gray-800 leading-relaxed">{questions[currentQuestionIndex]}</p>
              {!isVideoOn && (
                <p className="text-red-500 text-sm mt-2">Failed to play speech</p>
              )}
            </div>
          </div>

          {/* Violation Logs */}
          <div className="bg-white rounded-2xl shadow-lg p-4 mb-4">
            <div className="flex items-center justify-between mb-2">
              <h4 className="text-lg font-semibold text-gray-800">Proctoring Logs</h4>
              <div className="flex items-center space-x-2">
                <span className="text-xs text-gray-500">Face Detection: </span>
                <div className={`w-2 h-2 rounded-full ${facePresent ? 'bg-green-500' : 'bg-red-500 animate-pulse'}`}></div>
                <span className="text-xs ${facePresent ? 'text-green-600' : 'text-red-600 font-bold'}">{
                  facePresent ? 'Face Detected' : 'NO FACE DETECTED'
                }</span>
              </div>
            </div>
            
            <div className="max-h-48 overflow-y-auto border border-gray-200 rounded-lg p-2 bg-gray-50">
              {/* Debug info */}
              <div className="text-xs text-gray-500 mb-2">
                Logs count: {violationLogs.length} | Face present: {facePresent ? 'Yes' : 'No'} | Status: {faceDetectionStatus}
              </div>
              
              {violationLogs.length === 0 ? (
                <p className="text-sm text-gray-500 italic p-2">No violations detected</p>
              ) : (
                <div className="space-y-2">
                  {violationLogs.map((log) => (
                    <div key={log.id} className={`text-xs p-2 rounded flex items-start space-x-2 ${
                      log.type === 'error' ? 'bg-red-50 text-red-700 border border-red-200' : 
                      log.type === 'warning' ? 'bg-yellow-50 text-yellow-700 border border-yellow-200' : 
                      'bg-blue-50 text-blue-700 border border-blue-200'
                    }`}>
                      <div className="mt-0.5">
                        {log.type === 'error' ? (
                          <AlertTriangle className="w-4 h-4 text-red-600" />
                        ) : log.type === 'warning' ? (
                          <AlertTriangle className="w-4 h-4 text-yellow-600" />
                        ) : (
                          <div className="w-4 h-4 rounded-full bg-blue-500"></div>
                        )}
                      </div>
                      <div className="flex-1">
                        <div className="flex justify-between items-start">
                          <span className="font-medium">{log.message}</span>
                          <span className="text-xs opacity-75 ml-2 whitespace-nowrap">{log.timestamp}</span>
                        </div>
                        {log.duration && <span className="text-xs opacity-75 block mt-1">Duration: {log.duration}</span>}
                      </div>
                    </div>
                  ))}
                </div>
              )}
            </div>
          </div>
          
          {/* Response Area */}
          <div className="bg-white rounded-2xl shadow-lg p-6 flex-1">
            <h4 className="text-lg font-semibold text-gray-800 mb-4">Your Response</h4>
            <div className="space-y-3">
              <div className="flex items-center space-x-3">
                <div className="flex items-center space-x-2">
                  <Mic className="w-4 h-4 text-green-500" />
                  <span className="text-sm text-gray-600">Microphone active - speak your response</span>
                </div>
              </div>
              <div className="flex items-center space-x-2 text-green-600">
                <div className="w-2 h-2 bg-green-500 rounded-full"></div>
                <span className="text-sm">Good audio</span>
              </div>
              <div className="flex items-center space-x-2 text-gray-500">
                <span className="text-sm">No USB storage devices</span>
              </div>
            </div>
            
            <div className="mt-6 p-4 bg-gray-50 rounded-xl">
              <p className="text-gray-600 text-center">Your responses will appear here after you answer the question.</p>
            </div>
          </div>

          {/* End of main content */}
        </div>
      </div>

      {/* Completion Modal */}
      {isComplete && !showReport && (
        <div className="fixed inset-0 bg-black/50 flex items-center justify-center z-50">
          <div className="bg-white rounded-2xl p-8 max-w-md mx-4">
            <h3 className="text-2xl font-bold text-gray-800 mb-4">Practice Complete!</h3>
            <p className="text-gray-600 mb-6">
              Great job! You've completed your self-practice session. 
              {hasRecording && " Your interview has been recorded and is ready for download."}
            </p>
            <div className="flex flex-col space-y-3">
              <button
                onClick={() => setShowCustomReport(true)}
                className="px-4 py-2 bg-indigo-500 text-white rounded-lg hover:bg-indigo-600 transition-colors flex items-center justify-center"
              >
                <Download className="w-5 h-5 mr-2" />
                View Performance Report
              </button>
              {hasRecording && (
                <button
                  onClick={downloadRecording}
                  className="px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 transition-colors flex items-center justify-center"
                >
                  <Download className="w-5 h-5 mr-2" />
                  Download Recording
                </button>
              )}
              <button
                onClick={onBack}
                className="px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition-colors"
              >
                Continue
              </button>
            </div>
          </div>
        </div>
      )}

      {/* Self Practice Report Popup */}
      {showCustomReport && (
        <SelfPracticeReport
          onClose={() => setShowCustomReport(false)}
          userDetails={{
            fullName: userDetails.fullName,
            email: userDetails.email,
            phoneNumber: userDetails.phoneNumber,
            skills: userDetails.skills,
            experience: userDetails.experience
          }}
        />
      )}
    </div>
  );
};

export default Level4Flow;
